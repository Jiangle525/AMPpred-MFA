{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f22148d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from AMPpred_MFA.lib.Encoding import easy_encoding\n",
    "from AMPpred_MFA.lib.Vocab import load_vocab\n",
    "from AMPpred_MFA.lib.Data import load_dataset\n",
    "from AMPpred_MFA.models.Model import load_model\n",
    "from AMPpred_MFA.models.AMPpred_MFA import Model, Config\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams.update({'font.size': 16})  \n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "save_path_feature_visual = '../figures/feature visualization/1_trial' # 保存路径\n",
    "os.makedirs(save_path_feature_visual, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7c68c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36mLoading vocabulary...\u001b[0m\n",
      "\u001b[1;32mVocabulary has been loaded.\u001b[0m\n",
      "\u001b[0;36mLoading model...\u001b[0m\n",
      "\u001b[1;32mModel has been loaded.\u001b[0m\n",
      "Predicting dataset...\n",
      "Dataset has been predicted.\n"
     ]
    }
   ],
   "source": [
    "vocab_path = './trained_model/vocab.json'\n",
    "model_path = './trained_model/model.pth'\n",
    "vocab = load_vocab(vocab_path)\n",
    "config = Config()\n",
    "config.k_mer = 1\n",
    "config.batch_size = 32\n",
    "config.embed_padding_idx = vocab[config.padding_token]\n",
    "config.feature_dim = 400\n",
    "config.vocab_size = len(vocab)\n",
    "model = Model(config)\n",
    "load_model(model, model_path)\n",
    "model.eval()\n",
    "dataset = load_dataset('../dataset/train/1_trial/train.fasta')[:3000]\n",
    "fastas = dataset[:, :-1]\n",
    "labels = dataset[:, -1].astype(np.int64)\n",
    "data_x = easy_encoding(fastas, 'mixed', vocab,\n",
    "                        config.k_mer, config.padding_size)\n",
    "print('Predicting dataset...')\n",
    "out = model(data_x)\n",
    "print('Dataset has been predicted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfeffbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存至： ../figures/feature visualization/1_trial\\Before attention of fragment-level.png\n",
      "已保存至： ../figures/feature visualization/1_trial\\After attention of fragment-level.png\n",
      "已保存至： ../figures/feature visualization/1_trial\\Before attention of dipeptide-level.png\n",
      "已保存至： ../figures/feature visualization/1_trial\\After attention of dipeptide-level.png\n",
      "已保存至： ../figures/feature visualization/1_trial\\After concat.png\n",
      "已保存至： ../figures/feature visualization/1_trial\\After dense.png\n"
     ]
    }
   ],
   "source": [
    "s_size = 26\n",
    "alpha = 0.8\n",
    "figsize = (6, 6)\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "fig_before_attention2, ax_before_attention2 = plt.subplots(1, 1, figsize=figsize)\n",
    "before_attention2 = model.attention_wight2_inputs.cpu().detach().numpy().reshape(fastas.shape[0], -1)\n",
    "tsne_before_attention2 = tsne.fit_transform(before_attention2)\n",
    "pos_before_attention2 = tsne_before_attention2[np.where(labels == 1)]\n",
    "neg_before_attention2 = tsne_before_attention2[np.where(labels == 0)]\n",
    "ax_before_attention2.scatter(pos_before_attention2[:, 0], \n",
    "                             pos_before_attention2[:, 1], \n",
    "                             c='b', \n",
    "                             label='AMPs', \n",
    "                             alpha=alpha, \n",
    "                             s=s_size,\n",
    "                             edgecolor=\"none\")\n",
    "ax_before_attention2.scatter(neg_before_attention2[:, 0], \n",
    "                             neg_before_attention2[:, 1], \n",
    "                             c='r', \n",
    "                             label='Non-AMPs', \n",
    "                             alpha=alpha, \n",
    "                             s=s_size, \n",
    "                             edgecolor=\"none\")\n",
    "ax_before_attention2.legend()\n",
    "fig_before_attention2.tight_layout()\n",
    "save_path_before_attention2 = os.path.join(save_path_feature_visual, 'Before attention of fragment-level.png')\n",
    "fig_before_attention2.savefig(save_path_before_attention2, dpi=300, bbox_inches='tight')\n",
    "print('已保存至：', save_path_before_attention2)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig_after_attention2, ax_after_attention2 = plt.subplots(1, 1, figsize=figsize)\n",
    "after_attention2 = model.attention_wight2_outputs.cpu().detach().numpy().reshape(fastas.shape[0], -1)\n",
    "tsne_after_attention2 = tsne.fit_transform(after_attention2)\n",
    "pos_after_attention2 = tsne_after_attention2[np.where(labels == 1)]\n",
    "neg_after_attention2 = tsne_after_attention2[np.where(labels == 0)]\n",
    "ax_after_attention2.scatter(pos_after_attention2[:, 0], \n",
    "                            pos_after_attention2[:, 1], \n",
    "                            c='b', \n",
    "                            label='AMPs', \n",
    "                            alpha=alpha, \n",
    "                            s=s_size,\n",
    "                            edgecolor=\"none\")\n",
    "ax_after_attention2.scatter(neg_after_attention2[:, 0], \n",
    "                            neg_after_attention2[:, 1], \n",
    "                            c='r', \n",
    "                            label='Non-AMPs', \n",
    "                            alpha=alpha, \n",
    "                            s=s_size,\n",
    "                            edgecolor=\"none\")\n",
    "ax_after_attention2.legend()\n",
    "fig_after_attention2.tight_layout()\n",
    "save_path_after_attention2 = os.path.join(save_path_feature_visual, 'After attention of fragment-level.png')\n",
    "fig_after_attention2.savefig(save_path_after_attention2, dpi=300, bbox_inches='tight')\n",
    "print('已保存至：', save_path_after_attention2)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig_before_attention1, ax_before_attention1 = plt.subplots(1, 1, figsize=figsize)\n",
    "before_attention1 = model.attention_wight1_inputs.cpu().detach().numpy().reshape(fastas.shape[0], -1)\n",
    "tsne_before_attention1 = tsne.fit_transform(before_attention1)\n",
    "pos_before_attention1 = tsne_before_attention1[np.where(labels == 1)]\n",
    "neg_before_attention1 = tsne_before_attention1[np.where(labels == 0)]\n",
    "ax_before_attention1.scatter(pos_before_attention1[:, 0], \n",
    "                             pos_before_attention1[:, 1], \n",
    "                             c='b', \n",
    "                             label='AMPs', \n",
    "                             alpha=alpha, \n",
    "                             s=s_size,\n",
    "                             edgecolor=\"none\")\n",
    "ax_before_attention1.scatter(neg_before_attention1[:, 0], \n",
    "                             neg_before_attention1[:, 1], \n",
    "                             c='r', \n",
    "                             label='Non-AMPs', \n",
    "                             alpha=alpha, \n",
    "                             s=s_size,\n",
    "                             edgecolor=\"none\")\n",
    "ax_before_attention1.legend()\n",
    "fig_before_attention1.tight_layout()\n",
    "save_path_before_attention1 = os.path.join(save_path_feature_visual, 'Before attention of dipeptide-level.png')\n",
    "fig_before_attention1.savefig(save_path_before_attention1, dpi=300, bbox_inches='tight')\n",
    "print('已保存至：', save_path_before_attention1)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig_after_attention1, ax_after_attention1 = plt.subplots(1, 1, figsize=figsize)\n",
    "after_attention1 = model.attention_wight1_outputs.cpu().detach().numpy().reshape(fastas.shape[0], -1)\n",
    "tsne_after_attention1 = tsne.fit_transform(after_attention1)\n",
    "pos_after_attention1 = tsne_after_attention1[np.where(labels == 1)]\n",
    "neg_after_attention1 = tsne_after_attention1[np.where(labels == 0)]\n",
    "ax_after_attention1.scatter(pos_after_attention1[:, 0], \n",
    "                            pos_after_attention1[:, 1], \n",
    "                            c='b', \n",
    "                            label='AMPs', \n",
    "                            alpha=alpha, \n",
    "                            s=s_size,\n",
    "                            edgecolor=\"none\")\n",
    "ax_after_attention1.scatter(neg_after_attention1[:, 0], \n",
    "                            neg_after_attention1[:, 1],\n",
    "                            c='r', \n",
    "                            label='Non-AMPs', \n",
    "                            alpha=alpha, \n",
    "                            s=s_size,\n",
    "                            edgecolor=\"none\")\n",
    "ax_after_attention1.legend()\n",
    "fig_after_attention1.tight_layout()\n",
    "save_path_after_attention1 = os.path.join(save_path_feature_visual, 'After attention of dipeptide-level.png')\n",
    "fig_after_attention1.savefig(save_path_after_attention1, dpi=300, bbox_inches='tight')\n",
    "print('已保存至：', save_path_after_attention1)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig_after_concat, ax_after_concat = plt.subplots(1, 1, figsize=figsize)\n",
    "after_concat = model.last_feature.cpu().detach().numpy()\n",
    "tsne_after_concat = tsne.fit_transform(after_concat)\n",
    "pos_after_concat = tsne_after_concat[np.where(labels == 1)]\n",
    "neg_after_concat = tsne_after_concat[np.where(labels == 0)]\n",
    "ax_after_concat.scatter(pos_after_concat[:, 0], \n",
    "                        pos_after_concat[:, 1], \n",
    "                        c='b', \n",
    "                        label='AMPs', \n",
    "                        alpha=alpha, \n",
    "                        s=s_size,\n",
    "                        edgecolor=\"none\")\n",
    "ax_after_concat.scatter(neg_after_concat[:, 0], \n",
    "                        neg_after_concat[:, 1], \n",
    "                        c='r', \n",
    "                        label='Non-AMPs', \n",
    "                        alpha=alpha, \n",
    "                        s=s_size,\n",
    "                        edgecolor=\"none\")\n",
    "ax_after_concat.legend()\n",
    "fig_after_concat.tight_layout()\n",
    "save_path_after_concat = os.path.join(save_path_feature_visual, 'After concat.png')\n",
    "fig_after_concat.savefig(save_path_after_concat, dpi=300, bbox_inches='tight')\n",
    "print('已保存至：', save_path_after_concat)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig_after_dense, ax_after_dense = plt.subplots(1, 1, figsize=figsize)\n",
    "after_dense = out.cpu().detach().numpy()\n",
    "tsne_after_dense = tsne.fit_transform(after_dense)\n",
    "pos_after_dense = tsne_after_dense[np.where(labels == 1)]\n",
    "neg_after_dense = tsne_after_dense[np.where(labels == 0)]\n",
    "ax_after_dense.scatter(pos_after_dense[:, 0], \n",
    "                       pos_after_dense[:, 1], \n",
    "                       c='b', \n",
    "                       label='AMPs', \n",
    "                       alpha=alpha, \n",
    "                       s=s_size,\n",
    "                       edgecolor=\"none\")\n",
    "ax_after_dense.scatter(neg_after_dense[:, 0], \n",
    "                       neg_after_dense[:, 1], \n",
    "                       c='r', \n",
    "                       label='Non-AMPs', \n",
    "                       alpha=alpha, \n",
    "                       s=s_size, \n",
    "                       edgecolor=\"none\")\n",
    "ax_after_dense.legend()\n",
    "fig_after_dense.tight_layout()\n",
    "save_path_after_dense = os.path.join(save_path_feature_visual, 'After dense.png')\n",
    "fig_after_dense.savefig(save_path_after_dense, dpi=300, bbox_inches='tight')\n",
    "print('已保存至：', save_path_after_dense)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0612d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
