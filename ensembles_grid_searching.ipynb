{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of physical cores: 14\n"
     ]
    }
   ],
   "source": [
    "from AMPpred_MFA.lib.Data import *\n",
    "from AMPpred_MFA.lib.Visualization import colorful_print, current_time, draw_roc\n",
    "from AMPpred_MFA.lib.Encoding import AAC\n",
    "from AMPpred_MFA.lib.Visualization import *\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    StackingClassifier,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import math\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "CPU_NUM_CORES = joblib.cpu_count(only_physical_cores=True)\n",
    "print(f\"Number of physical cores: {CPU_NUM_CORES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_pos = './dataset/our_dataset/amps.fasta'\n",
    "file_path_neg = './dataset/our_dataset/non_amps.fasta'\n",
    "file_path_train = './dataset/train/1_trial/train.fasta'\n",
    "file_path_test = './dataset/test/our_testset/1_trial/test.fasta'\n",
    "\n",
    "# 使用GridSearchCV进行网格搜索\n",
    "def k_fold_grid_search(model, param_grid, X_train, y_train, k_fold=5):\n",
    "    scoring = 'accuracy'\n",
    "    grid_search = GridSearchCV(estimator=model,\n",
    "                               param_grid=param_grid,\n",
    "                               scoring=scoring,\n",
    "                               cv=k_fold,\n",
    "                               verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search\n",
    "\n",
    "X_train, y_train = build_dataset_from_format(file_path_train,\n",
    "                                             feature_function=AAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT = \"Decision Tree\"\n",
    "RF = 'Random Forest'\n",
    "ADABOOST = 'AdaBoost'\n",
    "XGBOOST = 'XGBoost'\n",
    "STACKING = 'Stacking'\n",
    "models = {\n",
    "    DT: DecisionTreeClassifier(),\n",
    "    RF: RandomForestClassifier(n_jobs=CPU_NUM_CORES),\n",
    "    ADABOOST: AdaBoostClassifier(),\n",
    "    XGBOOST: XGBClassifier(n_jobs=CPU_NUM_CORES),\n",
    "}\n",
    "\n",
    "params_grid = {\n",
    "     DT: {\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_depth\": [6, 10, 20, None],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "    },\n",
    "    RF: {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [6, 10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2', None],\n",
    "    },\n",
    "    ADABOOST: {\n",
    "        'n_estimators': [100, 150, 200, 300],\n",
    "        'algorithm': ['SAMME', 'SAMME.R'],\n",
    "        'learning_rate': [0.005, 0.01, 0.05, 0.1, 0.5]\n",
    "    },\n",
    "    XGBOOST: {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'gamma': [0, 0.5, 1],\n",
    "        'min_child_weight': [1, 3, 5, 10],\n",
    "        'max_depth': [6, 10, 20],\n",
    "        'subsample': [0.6, 0.8, 1],\n",
    "        'colsample_bytree': [0.6, 0.8, 1],\n",
    "        'learning_rate': [0.005, 0.01, 0.05, 0.1, 0.5]\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 216 candidates, totalling 648 fits\n",
      "Decision Tree's best param: {'criterion': 'gini', 'max_depth': 6, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "Fitting 3 folds for each of 648 candidates, totalling 1944 fits\n",
      "Random Forest's best param: {'criterion': 'entropy', 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n",
      "AdaBoost's best param: {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 300}\n",
      "Fitting 3 folds for each of 4860 candidates, totalling 14580 fits\n",
      "XGBoost's best param: {'colsample_bytree': 0.6, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 10, 'min_child_weight': 1, 'n_estimators': 300, 'subsample': 1}\n"
     ]
    }
   ],
   "source": [
    "grids_search = {}\n",
    "for name in models:\n",
    "    model = models[name]\n",
    "    param_grid = params_grid[name]\n",
    "    grid_search = k_fold_grid_search(\n",
    "        model, param_grid, X_train, y_train, k_fold=3)\n",
    "    grids_search[name] = grid_search\n",
    "    print(\"{}'s best param: {}\".format(name, grid_search.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
